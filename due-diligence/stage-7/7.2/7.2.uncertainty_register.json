{
  "sub_stage": "7.2",
  "document_metadata": {
    "title": "Data Lineage and Transformation Uncertainty Register",
    "analysis_date": "2026-02-15"
  },
  "uncertainty_register": [
    {
      "unknown": "CRM opportunity qualification criteria and transformation rules—what thresholds, filters, stage progression logic sales applies when entering/updating opportunities",
      "type": "UNKNOWN",
      "decision_impact": "Cannot quantify CRM pipeline inflation magnitude—if qualification criteria loose, pipeline systematically overstates viable opportunities, resource allocation (hiring, capacity planning) misaligned. Cannot assess walk away policy Q4 2024 necessity—if qualification already tight, CEO filtering indicates delivery-sales disconnect not sales optimism; if qualification loose, filtering corrects sales entry bias. Cannot evaluate sales refresh Q3 2024 effectiveness—if qualification criteria unchanged post-refresh, personnel change insufficient to resolve pipeline quality issues. Cannot predict which deals CEO will reject—if criteria undocumented (Stage 7.1 uncertainty), sales cannot self-filter creating downstream surprise and wasted sales effort.",
      "what_would_reduce_uncertainty": "CRM configuration documentation: Opportunity stages and required fields per stage, qualification criteria (BANT, MEDDIC, other methodology), close probability by stage, automated validation rules or manual judgment, stage progression approval workflows if exist. Sales methodology documentation: How are opportunities sourced and qualified? What information required before CRM entry? Who can create opportunities vs approve progression? Historical CRM data quality: What percentage of opportunities close? How long do opportunities remain in pipeline before win/loss/disqualification? What percentage of bookings later walked away (Q4 2024 policy implementation rate)? Sales compensation structure: What CRM metrics drive compensation—pipeline value, bookings, revenue? Incentive alignment with quality vs volume?"
    },
    {
      "unknown": "Finance conservative adjustment methodology—specific formulas, discount factors, judgment overlays finance applies when converting operational forecasts to investor guidance",
      "type": "UNKNOWN",
      "decision_impact": "Cannot assess forecast reliability—if adjustment methodology inconsistent or judgmental, forecast accuracy uncertain, investor confidence risk. Cannot quantify internal-external truth gap—if adjustment magnitude unknown, cannot determine whether operations optimistic or finance pessimistic or both. Cannot evaluate operations planning alignment—if operations plans based on unadjusted internal forecast while finance commits to adjusted external forecast, misalignment magnitude unknown creating resource mismatches. Cannot determine whether adjustment appropriate—if methodology opaque, cannot assess whether conservatism corrects operational bias or creates excessive pessimism suppressing growth investment.",
      "what_would_reduce_uncertainty": "Finance forecasting process documentation: How does finance receive operational forecasts (BU-level, function-level)? What adjustments applied systematically (discount factors, reserve additions, timing shifts)? What adjustments judgmental (CFO overlays based on experience, market conditions)? Adjustment by domain: CRM pipeline discount methodology (stage-based, product-based, jurisdiction-based)? Customer count adjustment (CRM to billing to revenue-active reconciliation)? Margin forecast adjustment (operational estimates to conservative external)? Historical accuracy: Finance-adjusted forecast vs actual results past 8 quarters? Operational forecast vs actual results (to isolate adjustment accuracy)? Adjustment trend: Is conservatism increasing or decreasing over time? Did adjustment methodology change with CFO transitions?"
    },
    {
      "unknown": "Multi-jurisdictional aggregation formulas—how jurisdiction-specific operational metrics combined into enterprise-level reporting",
      "type": "UNKNOWN",
      "decision_impact": "Cannot assess enterprise operational metrics validity—if aggregation methodology flawed (ignores non-fungibility, uses simple sum where weighted average appropriate, combines incompatible metrics), enterprise truth misleading and decisions misguided. Cannot evaluate investor communications accuracy—if aggregation presented without caveat about jurisdiction isolation (Stage 6 5-6x coordination multiplier), investors misunderstand operational reality. Cannot determine infrastructure investment appropriateness—if enterprise utilization calculated assuming fungible capacity, investment may be misallocated (expansion in low-utilization jurisdiction doesn't relieve high-utilization jurisdiction if capacity non-fungible). Cannot assess M&A integration complexity—if aggregation hides variance, acquirer integration planning based on false simplicity.",
      "what_would_reduce_uncertainty": "Operations reporting methodology: How are FedRAMP, UK, HIPAA, China, other jurisdiction metrics collected? What systems produce jurisdiction-level data? How is data aggregated for enterprise view—simple sum, weighted average, consolidated with eliminations? Capacity and utilization calculation: Are utilization rates calculated per jurisdiction then averaged, or capacity/usage summed then ratio calculated enterprise-wide? Does methodology account for capacity non-fungibility across jurisdictions? Is workload mobility across jurisdictions measured? Performance metrics: Are SLA, incident response, availability metrics aggregated by jurisdiction or customer population? Are multi-jurisdictional customers measured differently than single-jurisdiction? Variance disclosure: Are jurisdiction-specific metrics disclosed separately anywhere (investor presentations, SEC filings, internal Board materials) or only aggregates presented? Does investor/Board reporting note aggregation limitations or present as unified enterprise?"
    },
    {
      "unknown": "Manual override frequency and patterns—how often CEO, finance, operations manually adjust automated data flows and what triggers overrides",
      "type": "UNKNOWN",
      "decision_impact": "Cannot assess data consistency reliability—if manual overrides frequent, automated data untrustworthy creating dual truths (system of record vs manually adjusted). Cannot quantify walk away policy impact—if CEO filtering 5% vs 50% of deals, pipeline reliability and sales predictability materially different. Cannot evaluate finance adjustment predictability—if adjustment magnitude varies widely quarter-to-quarter based on CFO judgment, operational planning uncertainty high. Cannot determine automation feasibility—if overrides required because automated rules insufficient, systematization difficult without business logic improvement; if overrides discretionary (authority-driven not rule-driven), systematization requires authority delegation not technical enhancement.",
      "what_would_reduce_uncertainty": "Manual override tracking: Are overrides logged, tracked, analyzed? What percentage of CRM opportunities manually adjusted/filtered post-entry? What percentage of operational forecasts manually adjusted by finance before external reporting? What percentage of incident classifications manually changed post-initial assignment? Override triggers: What business conditions trigger CEO walk away decision—margin threshold, strategic misalignment, competitive dynamics? What conditions trigger finance conservative adjustment—operational forecast volatility, prior forecast miss, market uncertainty? What conditions trigger operations incident reclassification—customer escalation, regulatory inquiry, executive attention? Override authority: Who has override authority by domain—CEO only, finance only, BU leaders, operational managers? Are override decisions documented with rationale or judgment-only? Override outcomes: Do overrides improve accuracy (validated ex-post) or reflect political preferences? Historical pattern: Override frequency increasing or decreasing over time? Did override patterns change with CEO transitions (three in three years)?"
    },
    {
      "unknown": "Data quality at each transformation step—accuracy, completeness, timeliness of data inputs and outputs at T1, T2, T3, T4, T5 in transformation chains",
      "type": "UNKNOWN",
      "decision_impact": "Cannot assess where semantic drift originates—if data quality degraded at T1 (sales CRM entry inaccurate), downstream transformations compound error; if quality degraded at T3 (finance adjustment excessive), source data adequate but transformation flawed. Cannot prioritize data quality investment—if T1 quality lowest, investment in CRM data governance highest ROI; if T5 quality lowest, investment in executive decision support highest ROI. Cannot evaluate transformation fault tolerance—if data quality measured at each step, can determine whether transformations preserve/degrade/improve quality; if unmeasured, transformation impact on quality unknown. Cannot assess M&A data quality claims—if quality not measured systematically, claims to acquirer about data accuracy unverifiable.",
      "what_would_reduce_uncertainty": "Data quality measurement infrastructure: Are data quality metrics defined and tracked—accuracy (how often is data correct?), completeness (what percentage of required fields populated?), timeliness (how fresh is data?), consistency (do related systems agree?)? Quality by transformation step: What is CRM data quality at entry (T1)—deal sizing accuracy, close date accuracy, customer information accuracy? What is delivery cost data quality (T3)—time tracking accuracy, cost allocation accuracy? What is finance reporting quality (T4)—reconciliation to source systems, audit findings? Quality monitoring: Are quality metrics reviewed regularly—monthly, quarterly? Who monitors quality—data quality team, function owners, audit? Are quality issues escalated and resolved systematically? Quality investment history: Has data quality improved over time or degraded? What investments made in data quality tools, processes, governance? What ROI achieved from quality investments?"
    },
    {
      "unknown": "Transformation ownership and accountability—who performs each transformation step, with what authority, and with what accountability for accuracy",
      "type": "UNKNOWN",
      "decision_impact": "Cannot assess whether transformation control appropriate—if sales controls CRM qualification without delivery input, optimism bias predictable; if finance controls adjustment without operations input, conservatism bias predictable; if operations controls incident classification without customer input, severity understatement predictable. Cannot evaluate governance solution feasibility—if transformation owners identified, governance can assign accountability; if ownership unclear or contested, governance design difficult. Cannot determine where transformation disputes arise—if ownership boundaries clear, disputes occur at interfaces (sales-delivery, operations-finance); if boundaries unclear, disputes pervasive. Cannot assess M&A integration governance—if transformation ownership stable and documented, acquirer can evaluate; if ownership informal or person-dependent (three CEOs three years), integration requires governance rebuild.",
      "what_would_reduce_uncertainty": "Transformation RACI or ownership documentation: For each major transformation (CRM qualification, finance adjustment, incident classification, multi-jurisdictional aggregation, CEO filtering): Who is responsible for performing transformation? Who has authority to change transformation methodology? Who must be consulted before transformation changes? Who is informed of transformation outputs? Accountability definition: Are transformation owners accountable for accuracy—measured how, validated how, consequences for inaccuracy? Are transformation owners accountable for downstream impacts—margin erosion from CRM optimism, resource misallocation from finance conservatism? Cross-functional governance: Do transformations have cross-functional oversight—operating committee, data governance committee, deal review board? Or unilateral by function? Transformation disputes: How are transformation disputes resolved—escalate to CEO, negotiate between functions, data-driven methodology assessment? Dispute frequency: How often do transformation methodologies get contested—rare or routine?"
    },
    {
      "unknown": "Temporal lag patterns—average time between data creation and data consumption for material decisions, and whether lag increasing or stable",
      "type": "UNKNOWN",
      "decision_impact": "Cannot assess temporal drift severity—if lag short (days), temporal drift minimal; if lag long (months), semantic staleness severe in volatile environment (margin erosion, satisfaction decline). Cannot evaluate decision tempo appropriateness—if decisions delayed weeks-months while data ages, decision quality degraded; if decisions rapid despite lag, may be acting on stale reality. Cannot determine whether lag structural or episodic—if lag stable, may be inherent in measurement cadence (monthly close, quarterly review); if lag increasing, may indicate coordination bottleneck worsening (Stage 4.2 CEO bottleneck). Cannot assess whether lag explains reactive interventions—walk away policy Q4 2024 and sales refresh Q3 2024 reactive, may be discovering deterioration months after reality changed due to measurement/decision lag.",
      "what_would_reduce_uncertainty": "Data vintage tracking: For CRM opportunities, what is average age—how long between opportunity creation and CEO review for walk away assessment? For financial forecasts, what is vintage—how old is pipeline snapshot when quarterly guidance issued? For customer satisfaction, what is lag—how long between service interaction and satisfaction reporting to executives? For incident root cause, what is lag—how long between incident occurrence and prevention decision? Decision tempo measurement: How long between data availability and executive decision? What causes delays—data aggregation/transformation time, executive calendar constraints, analysis/review time? Lag trend: Is temporal lag increasing or decreasing over time? Did lag change with CEO transitions? Correlation analysis: Do reactive interventions (walk away policy, sales refresh) correlate with lag spikes—did CRM pipeline quality deteriorate months before CEO intervention? Did margin erosion occur months before walk away policy established?"
    }
  ],
  "synthesis_notes": "Uncertainties cluster around transformation methodology (CRM qualification criteria, finance adjustment formulas, aggregation methodology, override patterns), data quality measurement (accuracy/completeness/timeliness at each transformation step), transformation governance (ownership, accountability, cross-functional oversight), temporal patterns (lag between creation and consumption). Decision impact material: Cannot quantify semantic drift magnitude without methodology visibility, cannot assess transformation appropriateness without quality measurement, cannot evaluate governance solution without ownership clarity, cannot determine temporal drift severity without lag measurement. Uncertainties interconnected: Methodology uncertainty prevents quality assessment (can't measure accuracy without knowing intended meaning), quality uncertainty prevents governance accountability (can't hold owners accountable without measurement), governance uncertainty prevents methodology improvement (unclear who has authority to change formulas). M&A impact: Acquirer diligence likely to demand all uncertain elements—transformation methodology documentation, data quality metrics, governance RACI, temporal lag analysis. If Rackspace cannot produce (methodology undocumented, quality unmeasured, governance informal, lag untracked), creates valuation uncertainty or due diligence failure risk. Reducing uncertainty requires: IT and finance documentation access (transformation logic, aggregation formulas), governance documentation if exists (ownership, accountability, dispute resolution), historical analysis (quality trends, lag trends, override frequency patterns)—all likely exist internally in some form but require diligence data room access to surface."
}
